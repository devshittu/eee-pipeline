# Dockerfile_event_llm
# File path: Dockerfile_event_llm

# Use NVIDIA CUDA base image for GPU support
ARG CUDA_VERSION="12.3.2"
ARG CUDNN_VERSION="9"
FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-cudnn${CUDNN_VERSION}-runtime-ubuntu22.04 AS base

# Set environment variables
ENV PYTHONUNBUFFERED 1
ENV DEBIAN_FRONTEND noninteractive

# Install common dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.10 python3.10-venv python3-pip \
    curl build-essential && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Create a virtual environment
ENV VIRTUAL_ENV=/opt/venv
RUN python3.10 -m venv $VIRTUAL_ENV
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Stage 1: Build stage - Install dependencies
FROM base AS build

WORKDIR /app

# Copy only requirements to leverage Docker cache
COPY requirements_event_llm.txt .

# Install Python dependencies
# Use --extra-index-url for PyTorch if needed for specific CUDA versions
RUN pip install --no-cache-dir -r requirements_event_llm.txt

# Stage 2: Final stage - Copy application code and runtime dependencies
FROM base AS final

WORKDIR /app

# Copy virtual environment from build stage
COPY --from=build ${VIRTUAL_ENV} ${VIRTUAL_ENV}

# Copy application code
COPY src ./src
COPY config ./config

# Set LLM model cache directory
ENV LLM_MODEL_PATH="/app/cache/llm"
# Create cache directory and ensure permissions
RUN mkdir -p /app/cache/llm && chmod -R 777 /app/cache

# Expose the port the service will run on
EXPOSE 8003

# Copy and set up entrypoint script
COPY entrypoint_event_llm.sh .
RUN chmod +x entrypoint_event_llm.sh

# Set the entrypoint
ENTRYPOINT ["./entrypoint_event_llm.sh"]

# Command to run the application (will be called by entrypoint)
CMD ["uvicorn", "src.api.event_llm_service:app", "--host", "0.0.0.0", "--port", "8003"]