# src/core/llm_postprocessor.py
# File path: src/core/llm_postprocessor.py

import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
from src.schemas.data_models import EventLLMGenerateResponse, Event, EventMetadata

logger = logging.getLogger("llm_postprocessor")


class LLMPostProcessor:
    """
    Handles post-generation cleanup, field mapping, and normalization of model output
    to ensure the final output strictly adheres to the schema.

    CRITICAL CHANGE: Removed aggressive imputation logic to avoid polluting downstream
    analytics with false sentiments/causality. Now, it only cleans empty strings,
    allowing truly missing data to remain 'null' as per the schema definition.
    
    NEW: Handles injection of pipeline-level metadata (document_id, normalized_date)
    that are not generated by the LLM but are required by the schema.
    """

    @staticmethod
    def map_and_normalize_data(data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Recursively maps inconsistent field names and performs type normalization 
        before Pydantic validation.
        
        CRITICAL: Maps LLM output keys to schema-expected keys:
        - "entities" -> "extracted_entities"
        - "soa_triplets" -> "extracted_soa_triplets"
        - "predicate" -> "action"
        """
        if isinstance(data, dict):
            new_data = {}
            for k, v in data.items():
                new_k = k

                # Map top-level response keys to schema-expected names
                if k == 'entities':
                    new_k = 'extracted_entities'
                elif k == 'soa_triplets':
                    new_k = 'extracted_soa_triplets'
                # Map nested predicate to action
                elif k == 'predicate':
                    new_k = 'action'

                # Recursively process nested structures
                processed_v = LLMPostProcessor.map_and_normalize_data(v)

                # Handle list vs. single entity for EventArgument flexibility
                if k == 'entities' and isinstance(processed_v, list) and new_k == 'entities':
                    # This is within an EventArgument, not the top level
                    new_data['entities'] = processed_v
                elif k == 'entity' and processed_v is not None:
                    if isinstance(processed_v, list):
                        new_data['entities'] = processed_v
                    else:
                        new_data['entity'] = processed_v
                else:
                    new_data[new_k] = processed_v

            # Final check for entity/entities at argument level
            if "entities" not in new_data and "entity" in new_data and isinstance(new_data["entity"], list):
                new_data["entities"] = new_data["entity"]
                del new_data["entity"]

            return new_data

        elif isinstance(data, list):
            return [LLMPostProcessor.map_and_normalize_data(item) for item in data]

        elif data == "" or data is None:
            return None

        return data

    @staticmethod
    def post_process_response(
        raw_json_data: Dict[str, Any],
        document_id: Optional[str] = None,
        normalized_date: Optional[str] = None,
        event_references: Optional[List[str]] = None
    ) -> EventLLMGenerateResponse:
        """
        Executes the full post-processing pipeline: mapping and validation.
        
        NEW: Accepts pipeline-level metadata that isn't generated by LLM.
        These fields are injected after mapping but before Pydantic validation
        to avoid requiring them in LLM output.
        
        Args:
            raw_json_data: Raw JSON output from LLM
            document_id: Unique document identifier (injected by pipeline)
            normalized_date: ISO-8601 normalized publication date (injected by pipeline)
            event_references: Related event IDs (injected by event linker)
        
        Returns:
            Validated EventLLMGenerateResponse with all metadata populated
        """
        # 1. Map and Normalize (Handles "predicate" -> "action" and ensures lists are nested correctly)
        cleaned_data = LLMPostProcessor.map_and_normalize_data(raw_json_data)

        # 2. Inject pipeline-level metadata with safe defaults
        # These fields are NOT part of LLM output but are required by schema
        import hashlib

        # Generate fallback document_id if not provided
        if document_id is None:
            # Use text hash as deterministic fallback
            text = cleaned_data.get('original_text', '')
            if text:
                document_id = hashlib.sha256(text.encode()).hexdigest()[:16]
            else:
                # Last resort: use job_id if available
                document_id = cleaned_data.get('job_id', 'unknown')
                logger.warning(
                    f"No document_id provided and no text available for hash. Using job_id as fallback: {document_id}")

        # CRITICAL: Inject required fields before Pydantic validation
        cleaned_data['document_id'] = document_id
        # Can be None per schema (Optional field)
        cleaned_data['normalized_date'] = normalized_date
        # Default to empty list
        cleaned_data['event_references'] = event_references or []

        logger.debug(
            f"Injected pipeline metadata: document_id={document_id}, "
            f"normalized_date={normalized_date}, "
            f"event_references={len(event_references) if event_references else 0} refs"
        )

        # 3. Validate against Pydantic schema
        # This step automatically sets missing optional fields (like metadata.sentiment) to None (null in JSON).
        try:
            validated_response = EventLLMGenerateResponse(**cleaned_data)
            logger.debug(
                f"Successfully validated response for document_id={document_id} with "
                f"{len(validated_response.events)} events"
            )
        except Exception as e:
            logger.error(
                f"Validation failed even after injecting metadata. "
                f"document_id={document_id}, error={e}",
                exc_info=True
            )
            logger.error(f"Cleaned data keys: {list(cleaned_data.keys())}")
            raise

        # NOTE: Imputation is intentionally removed here to honor the requirement
        # that fields should be truly null if the LLM cannot provide a value.

        return validated_response


# src/core/llm_postprocessor.py
# File path: src/core/llm_postprocessor.py
